{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proceedings classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i 'random_state.py'\n",
    "from packages import *\n",
    "from clean_functions import *\n",
    "from tokenizer import *\n",
    "from fit_models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary to store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results={}\n",
    "results['lstm']={}\n",
    "results['mlp']={}\n",
    "results['xgboost']={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers=[]\n",
    "\n",
    "#pd.DataFrame(index=index, columns=['modelo','neurons', 'lamb1', 'lamb2'])\n",
    "#hypers_nn=pd.DataFrame({'modelo':[None],'neurons':[None], 'lamb1':[None], 'lamb2':[None]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V/CNN/LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y=np.load('data/X_w2v.npy'),np.load('data/y_w2v.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode={'H:Arquivado': 1,'H:Ativo': 2,'H:Suspenso': 3}\n",
    "decode={1:'H:Arquivado',2:'H:Ativo',3:'H:Suspenso'}\n",
    "#\n",
    "for i in range(len(y)):\n",
    "    y[i]=encode[y[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4514, 5, 70, 100), (4514,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.array(y.astype(int))-1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=2/3, random_state=random_seed)\n",
    "\n",
    "y_train2=np.array(pd.get_dummies(y_train))\n",
    "y_val2=np.array(pd.get_dummies(y_val))\n",
    "y_test2=np.array(pd.get_dummies(y_test))\n",
    "\n",
    "np.shape(X_train),np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper=pd.read_csv('hyper/hyper_lstm_w2v')\n",
    "hyper=hyper.iloc[np.argsort(hyper.loc[:,'score']),:].tail(1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers.append(['lstm_w2v', hyper.loc[:,'ks'][0], hyper.loc[:,'neurons'][0], hyper.loc[:,'lamb1'][0], hyper.loc[:,'lamb2'][0],'-','-','-','-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=3\n",
    "Adam=optimizers.Adam(learning_rate=0.005, beta_1=0.9, beta_2=0.999, amsgrad=True)\n",
    "\n",
    "#Cleaning session\n",
    "tensorflow.keras.backend.clear_session()\n",
    " \n",
    "#Hyper\n",
    "k=hyper.loc[0,'ks']\n",
    "neuron=hyper.loc[0,'neurons']\n",
    "lamb1=hyper.loc[0,'lamb1']\n",
    "lamb2=hyper.loc[0,'lamb2']\n",
    "\n",
    "#Cleaning session\n",
    "tensorflow.keras.backend.clear_session()\n",
    "\n",
    "#Model for features extraction\n",
    "inputs = Input(shape=np.shape(X_train)[1:])\n",
    "conv = TimeDistributed(Conv1D(k, 1, activation='linear', kernel_constraint=unit_norm(axis=1), use_bias=False))(inputs)\n",
    "pool = TimeDistributed(GlobalMaxPooling1D())(conv)\n",
    "model_feat = Model(inputs, pool)\n",
    "\n",
    "#Model for classification\n",
    "pooled_inputs = Input(shape=(5, k))\n",
    "lstm = LSTM(neuron, kernel_regularizer=regularizers.l1_l2(lamb1, lamb2))(pooled_inputs)\n",
    "soft = Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l1_l2(lamb1, lamb2))(lstm)\n",
    "model_classific = Model(pooled_inputs, soft)\n",
    "\n",
    "#Final model\n",
    "outputs = model_classific(model_feat(inputs))\n",
    "model_cnn = Model(inputs, outputs)\n",
    "\n",
    "#Compiling\n",
    "model_cnn.compile(loss='categorical_crossentropy', optimizer=Adam, metrics=['accuracy'])\n",
    "\n",
    "#Running\n",
    "modelo=model_cnn.fit(X_train, y_train2, epochs=50,\n",
    "                                              batch_size=500,\n",
    "                                              shuffle=True,\n",
    "                                              verbose=False,\n",
    "                                              validation_data=(X_val, y_val2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.save('models/model_lstm_w2v.h5')\n",
    "model_feat.save('models/model_feat_lstm_w2v.h5')\n",
    "model_classific.save('models/model_classific_lstm_w2v.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n"
     ]
    }
   ],
   "source": [
    "results['lstm']['w2v']=bootstrap(X_test, y_test, model_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT/LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y=np.load('data/X_bert.npy'),np.load('data/y_bert.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode={'H:Arquivado': 1,'H:Ativo': 2,'H:Suspenso': 3}\n",
    "decode={1:'H:Arquivado',2:'H:Ativo',3:'H:Suspenso'}\n",
    "\n",
    "for i in range(len(y)):\n",
    "    y[i]=encode[y[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4514, 5, 768), (4514,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.array(y.astype(int))-1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=2/3, random_state=random_seed)\n",
    "\n",
    "y_train2=np.array(pd.get_dummies(y_train))\n",
    "y_val2=np.array(pd.get_dummies(y_val))\n",
    "y_test2=np.array(pd.get_dummies(y_test))\n",
    "\n",
    "np.shape(X_train),np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper=pd.read_csv('hyper/hyper_lstm_bert')\n",
    "hyper=hyper.iloc[np.argsort(hyper.loc[:,'score']),:].tail(1).reset_index()\n",
    "model=model_lstm(hyper, X_train, y_train2, X_val, y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers.append(['lstm_bert', '-', hyper.loc[:,'neurons'][0], hyper.loc[:,'lamb1'][0], hyper.loc[:,'lamb2'][0],'-','-','-','-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:13<00:00,  7.20it/s]\n"
     ]
    }
   ],
   "source": [
    "results['lstm']['bert']=bootstrap(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec/LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y=np.load('data/X_d2v.npy'),np.load('data/y_d2v.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode={'H:Arquivado': 1,'H:Ativo': 2,'H:Suspenso': 3}\n",
    "decode={1:'H:Arquivado',2:'H:Ativo',3:'H:Suspenso'}\n",
    "\n",
    "for i in range(len(y)):\n",
    "    y[i]=encode[y[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4514, 5, 100), (4514,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.array(y.astype(int))-1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=2/3, random_state=random_seed)\n",
    "\n",
    "y_train2=np.array(pd.get_dummies(y_train))\n",
    "y_val2=np.array(pd.get_dummies(y_val))\n",
    "y_test2=np.array(pd.get_dummies(y_test))\n",
    "\n",
    "np.shape(X_train),np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper=pd.read_csv('hyper/hyper_lstm_d2v')\n",
    "hyper=hyper.iloc[np.argsort(hyper.loc[:,'score']),:].tail(1).reset_index()\n",
    "model=model_lstm(hyper, X_train, y_train2, X_val, y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers.append(['lstm_d2v',  '-', hyper.loc[:,'neurons'][0], hyper.loc[:,'lamb1'][0], hyper.loc[:,'lamb2'][0],'-','-','-','-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 13.32it/s]\n"
     ]
    }
   ],
   "source": [
    "results['lstm']['d2v']=bootstrap(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF/LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y=np.load('data/X_tfidf.npy'),np.load('data/y_tfidf.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode={'H:Arquivado': 1,'H:Ativo': 2,'H:Suspenso': 3}\n",
    "decode={1:'H:Arquivado',2:'H:Ativo',3:'H:Suspenso'}\n",
    "\n",
    "for i in range(len(y)):\n",
    "    y[i]=encode[y[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4514, 5, 4000), (4514,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.array(y.astype(int))-1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=2/3, random_state=random_seed)\n",
    "\n",
    "y_train2=np.array(pd.get_dummies(y_train))\n",
    "y_val2=np.array(pd.get_dummies(y_val))\n",
    "y_test2=np.array(pd.get_dummies(y_test))\n",
    "\n",
    "np.shape(X_train),np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper=pd.read_csv('hyper/hyper_lstm_tfidf')\n",
    "hyper=hyper.iloc[np.argsort(hyper.loc[:,'score']),:].tail(1).reset_index()\n",
    "model=model_lstm(hyper, X_train, y_train2, X_val, y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers.append(['lstm_tfidf',  '-', hyper.loc[:,'neurons'][0], hyper.loc[:,'lamb1'][0], hyper.loc[:,'lamb2'][0],'-','-','-','-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:46<00:00,  2.15it/s]\n"
     ]
    }
   ],
   "source": [
    "results['lstm']['tfidf']=bootstrap(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V/MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y=np.load('data/X_w2v.npy'),np.load('data/y_w2v.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=np.sum(X[:,:,:,0]!=0, axis=2)\n",
    "N=np.expand_dims(N, axis=2)\n",
    "N[N==0]=1\n",
    "X=np.sum(X, axis=2)\n",
    "X=X/N\n",
    "X=X.reshape((X.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode={'H:Arquivado': 1,'H:Ativo': 2,'H:Suspenso': 3}\n",
    "decode={1:'H:Arquivado',2:'H:Ativo',3:'H:Suspenso'}\n",
    "\n",
    "for i in range(len(y)):\n",
    "    y[i]=encode[y[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4514, 500), (4514,))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.array(y.astype(int))-1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=2/3, random_state=random_seed)\n",
    "\n",
    "y_train2=np.array(pd.get_dummies(y_train))\n",
    "y_val2=np.array(pd.get_dummies(y_val))\n",
    "y_test2=np.array(pd.get_dummies(y_test))\n",
    "\n",
    "np.shape(X_train),np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper=pd.read_csv('hyper/hyper_mlp_w2v')\n",
    "hyper=hyper.iloc[np.argsort(hyper.loc[:,'score']),:].tail(1).reset_index()\n",
    "model=model_mlp(hyper, X_train, y_train2, X_val, y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers.append(['mlp_w2v',  '-', hyper.loc[:,'neurons'][0], hyper.loc[:,'lamb1'][0], hyper.loc[:,'lamb2'][0],'-','-','-','-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.38it/s]\n"
     ]
    }
   ],
   "source": [
    "results['mlp']['w2v']=bootstrap(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT/MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = np.load('data/X_bert.npy'),np.load('data/y_bert.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.reshape((X.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode={'H:Arquivado': 1,'H:Ativo': 2,'H:Suspenso': 3}\n",
    "decode={1:'H:Arquivado',2:'H:Ativo',3:'H:Suspenso'}\n",
    "\n",
    "for i in range(len(y)):\n",
    "    y[i]=encode[y[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4514, 3840), (4514,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.array(y.astype(int))-1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=2/3, random_state=random_seed)\n",
    "\n",
    "y_train2=np.array(pd.get_dummies(y_train))\n",
    "y_val2=np.array(pd.get_dummies(y_val))\n",
    "y_test2=np.array(pd.get_dummies(y_test))\n",
    "\n",
    "np.shape(X_train),np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper=pd.read_csv('hyper/hyper_mlp_bert')\n",
    "hyper=hyper.iloc[np.argsort(hyper.loc[:,'score']),:].tail(1).reset_index()\n",
    "model=model_mlp(hyper, X_train, y_train2, X_val, y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers.append(['mlp_bert',  '-', hyper.loc[:,'neurons'][0], hyper.loc[:,'lamb1'][0], hyper.loc[:,'lamb2'][0],'-','-','-','-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:11<00:00,  8.35it/s]\n"
     ]
    }
   ],
   "source": [
    "results['mlp']['bert']=bootstrap(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec/MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y=np.load('data/X_d2v.npy'),np.load('data/y_d2v.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.reshape((X.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode={'H:Arquivado': 1,'H:Ativo': 2,'H:Suspenso': 3}\n",
    "decode={1:'H:Arquivado',2:'H:Ativo',3:'H:Suspenso'}\n",
    "\n",
    "for i in range(len(y)):\n",
    "    y[i]=encode[y[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4514, 500), (4514,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.array(y.astype(int))-1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=2/3, random_state=random_seed)\n",
    "\n",
    "y_train2=np.array(pd.get_dummies(y_train))\n",
    "y_val2=np.array(pd.get_dummies(y_val))\n",
    "y_test2=np.array(pd.get_dummies(y_test))\n",
    "\n",
    "np.shape(X_train),np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper=pd.read_csv('hyper/hyper_mlp_d2v')\n",
    "hyper=hyper.iloc[np.argsort(hyper.loc[:,'score']),:].tail(1).reset_index()\n",
    "model=model_mlp(hyper, X_train, y_train2, X_val, y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers.append(['mlp_d2v',  '-', hyper.loc[:,'neurons'][0], hyper.loc[:,'lamb1'][0], hyper.loc[:,'lamb2'][0],'-','-','-','-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.51it/s]\n"
     ]
    }
   ],
   "source": [
    "results['mlp']['d2v']=bootstrap(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF/MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y=np.load('data/X_tfidf.npy'),np.load('data/y_tfidf.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.reshape((X.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode={'H:Arquivado': 1,'H:Ativo': 2,'H:Suspenso': 3}\n",
    "decode={1:'H:Arquivado',2:'H:Ativo',3:'H:Suspenso'}\n",
    "\n",
    "for i in range(len(y)):\n",
    "    y[i]=encode[y[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4514, 20000), (4514,))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.array(y.astype(int))-1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=2/3, random_state=random_seed)\n",
    "\n",
    "y_train2=np.array(pd.get_dummies(y_train))\n",
    "y_val2=np.array(pd.get_dummies(y_val))\n",
    "y_test2=np.array(pd.get_dummies(y_test))\n",
    "\n",
    "np.shape(X_train),np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper=pd.read_csv('hyper/hyper_mlp_tfidf')\n",
    "hyper=hyper.iloc[np.argsort(hyper.loc[:,'score']),:].tail(1).reset_index()\n",
    "model=model_mlp(hyper, X_train, y_train2, X_val, y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers.append(['mlp_tfidf',  '-', hyper.loc[:,'neurons'][0], hyper.loc[:,'lamb1'][0], hyper.loc[:,'lamb2'][0],'-','-','-','-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:31<00:00,  3.22it/s]\n"
     ]
    }
   ],
   "source": [
    "results['mlp']['tfidf']=bootstrap(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V/XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y=np.load('data/X_w2v.npy'),np.load('data/y_w2v.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=np.sum(X[:,:,:,0]!=0, axis=2)\n",
    "N=np.expand_dims(N, axis=2)\n",
    "N[N==0]=1\n",
    "X=np.sum(X, axis=2)\n",
    "X=X/N\n",
    "X=X.reshape((X.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode={'H:Arquivado': 1,'H:Ativo': 2,'H:Suspenso': 3}\n",
    "decode={1:'H:Arquivado',2:'H:Ativo',3:'H:Suspenso'}\n",
    "\n",
    "for i in range(len(y)):\n",
    "    y[i]=encode[y[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4514, 500), (4514,))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.array(y.astype(int))-1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=2/3, random_state=random_seed)\n",
    "\n",
    "np.shape(X_train),np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper=pd.read_csv('hyper/hyper_xgboost_w2v')\n",
    "hyper=hyper.iloc[np.argsort(hyper.loc[:,'score']),:].tail(1).reset_index()\n",
    "model=model_xgboost(hyper, X_train, y_train, X_val, y_val, random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers.append(['xgboost_w2v', '-', '-','-','-', hyper.loc[:,'max_depth'][0], \n",
    "               hyper.loc[:,'learning_rate'][0], hyper.loc[:,'gamma'][0], hyper.loc[:,'reg_lambda'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 113.07it/s]\n"
     ]
    }
   ],
   "source": [
    "results['xgboost']['w2v']=bootstrap(X_test, y_test, model, nn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT/XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = np.load('data/X_bert.npy'),np.load('data/y_bert.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.reshape((X.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode={'H:Arquivado': 1,'H:Ativo': 2,'H:Suspenso': 3}\n",
    "decode={1:'H:Arquivado',2:'H:Ativo',3:'H:Suspenso'}\n",
    "\n",
    "for i in range(len(y)):\n",
    "    y[i]=encode[y[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4514, 3840), (4514,))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.array(y.astype(int))-1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=2/3, random_state=random_seed)\n",
    "\n",
    "np.shape(X_train),np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper=pd.read_csv('hyper/hyper_xgboost_bert')\n",
    "hyper=hyper.iloc[np.argsort(hyper.loc[:,'score']),:].tail(1).reset_index()\n",
    "model=model_xgboost(hyper, X_train, y_train, X_val, y_val, random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers.append(['xgboost_bert', '-', '-','-','-', hyper.loc[:,'max_depth'][0], \n",
    "               hyper.loc[:,'learning_rate'][0], hyper.loc[:,'gamma'][0], hyper.loc[:,'reg_lambda'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 16.28it/s]\n"
     ]
    }
   ],
   "source": [
    "results['xgboost']['bert']=bootstrap(X_test, y_test, model, nn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec/XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y=np.load('data/X_d2v.npy'),np.load('data/y_d2v.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.reshape((X.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode={'H:Arquivado': 1,'H:Ativo': 2,'H:Suspenso': 3}\n",
    "decode={1:'H:Arquivado',2:'H:Ativo',3:'H:Suspenso'}\n",
    "\n",
    "for i in range(len(y)):\n",
    "    y[i]=encode[y[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4514, 500), (4514,))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.array(y.astype(int))-1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=2/3, random_state=random_seed)\n",
    "\n",
    "np.shape(X_train),np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper=pd.read_csv('hyper/hyper_xgboost_d2v')\n",
    "hyper=hyper.iloc[np.argsort(hyper.loc[:,'score']),:].tail(1).reset_index()\n",
    "model=model_xgboost(hyper, X_train, y_train, X_val, y_val, random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers.append(['xgboost_d2v', '-', '-','-','-', hyper.loc[:,'max_depth'][0], \n",
    "               hyper.loc[:,'learning_rate'][0], hyper.loc[:,'gamma'][0], hyper.loc[:,'reg_lambda'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 110.58it/s]\n"
     ]
    }
   ],
   "source": [
    "results['xgboost']['d2v']=bootstrap(X_test, y_test, model, nn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TFIDF/XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y=np.load('data/X_tfidf.npy'),np.load('data/y_tfidf.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.reshape((X.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode={'H:Arquivado': 1,'H:Ativo': 2,'H:Suspenso': 3}\n",
    "decode={1:'H:Arquivado',2:'H:Ativo',3:'H:Suspenso'}\n",
    "\n",
    "for i in range(len(y)):\n",
    "    y[i]=encode[y[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4514, 20000), (4514,))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.array(y.astype(int))-1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=2/3, random_state=random_seed)\n",
    "\n",
    "np.shape(X_train),np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper=pd.read_csv('hyper/hyper_xgboost_tfidf')\n",
    "hyper=hyper.iloc[np.argsort(hyper.loc[:,'score']),:].tail(1).reset_index()\n",
    "model=model_xgboost(hyper, X_train, y_train, X_val, y_val, random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers.append(['xgboost_tfidf', '-', '-','-','-', hyper.loc[:,'max_depth'][0], \n",
    "               hyper.loc[:,'learning_rate'][0], hyper.loc[:,'gamma'][0], hyper.loc[:,'reg_lambda'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:31<00:00,  3.16it/s]\n"
     ]
    }
   ],
   "source": [
    "results['xgboost']['tfidf']=bootstrap(X_test, y_test, model, nn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latex tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers=pd.DataFrame(hypers, columns=['model','ks','neurons','lamb1','lamb2','max_depth','learning_rate','gamma','reg_lambda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table*}[h] \n",
      " \\centering \n",
      " \\caption{Best values for hyperparameters} \n",
      "  \\begin{tabular}{c|c|cccc|cccc|c} \n",
      "  \\hline \n",
      "  Classifier & Feature extraction & Filters & Hidden Size & Reg l1 & Reg l2 & Max. depth & Learning rate & Gamma & Lambda & \\\\ \n",
      "  \\hline\n",
      "\n",
      "\\multirow{4}{*}{LSTM}\n",
      "& W2V & 9 &100 & 0.0005 & 0.0001 & - & - & - & - \\\\\n",
      "& Doc2Vec & - &25 & 5e-06 & 1e-05 & - & - & - & - \\\\\n",
      "& TFIDF & - &25 & 0.0001 & 1e-06 & - & - & - & - \\\\\n",
      "& BERT & - &150 & 0.001 & 0.0005 & - & - & - & - \\\\\n",
      "\\hline \n",
      "\n",
      "\n",
      "\\multirow{4}{*}{MLP}\n",
      "& W2V & - &200 & 0.0001 & 1e-05 & - & - & - & - \\\\\n",
      "& Doc2Vec & - &200 & 0.0001 & 0.0001 & - & - & - & - \\\\\n",
      "& TFIDF & - &200 & 0.0001 & 1e-06 & - & - & - & - \\\\\n",
      "& BERT & - &50 & 0.0005 & 1e-05 & - & - & - & - \\\\\n",
      "\\hline \n",
      "\n",
      "\n",
      "\\multirow{4}{*}{XGboost}\n",
      "& W2V & - &- & - & - & 5 & 1.0 & 1e-05 & 300.0 \\\\\n",
      "& Doc2Vec & - &- & - & - & 5 & 1.0 & 0.01 & 50.0 \\\\\n",
      "& TFIDF & - &- & - & - & 7 & 1.0 & 0.01 & 300.0 \\\\\n",
      "& BERT & - &- & - & - & 5 & 0.5 & 0.0001 & 10.0 \\\\\n",
      "\\hline \n",
      "\n",
      " \\end{tabular}% \n",
      " \\label{tab:hyper}% \n",
      " \\end{table*}% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\\begin{table*}[h] \\n\",\n",
    "      \"\\centering \\n\",\n",
    "      '\\caption{Best values for hyperparameters} \\n',\n",
    "      ' \\\\begin{tabular}{c|c|cccc|cccc|c} \\n',\n",
    "      ' \\\\hline \\n',\n",
    "      ' Classifier & Feature extraction & Filters & Hidden Size & Reg l1 & Reg l2 & Max. depth & Learning rate & Gamma & Lambda & \\\\\\ \\n',\n",
    "      ' \\\\hline')\n",
    "\n",
    "i=0\n",
    "for c in ['LSTM','MLP','XGboost']:\n",
    "    print('\\n\\\\multirow{4}{*}{'+c+'}')\n",
    "    \n",
    "    for f in ['W2V','Doc2Vec','TFIDF','BERT']:\n",
    "        \n",
    "        print('& '+f+' & '+str(hypers.iloc[i,1])+' &' +str(hypers.iloc[i,2])+' & '+str(hypers.iloc[i,3])+' & '+str(hypers.iloc[i,4])+' & '+str(hypers.iloc[i,5])+' & '+str(hypers.iloc[i,6])+' & '+str(hypers.iloc[i,7])+' & '+str(hypers.iloc[i,8]),\"\\\\\\\\\")\n",
    "        \n",
    "        i=i+1\n",
    "    print('\\\\hline \\n')\n",
    "    \n",
    "print(' \\end{tabular}% \\n',\n",
    "      '\\label{tab:hyper}% \\n',\n",
    "   '\\end{table*}% \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in set(results):\n",
    "    for j in set(results[i]):\n",
    "        results[i][j]['accuracy'][0]=np.round(results[i][j]['accuracy'][0], 2)\n",
    "        results[i][j]['accuracy'][1]=np.round(results[i][j]['accuracy'][1], 2)\n",
    "        for k in ['macro avg', 'weighted avg']:\n",
    "            results[i][j][k]['f1-score'][0]=np.round(results[i][j][k]['f1-score'][0], 2)\n",
    "            results[i][j][k]['f1-score'][1]=np.round(results[i][j][k]['f1-score'][1], 2)\n",
    "            results[i][j][k]['precision'][0]=np.round(results[i][j][k]['precision'][0], 2)\n",
    "            results[i][j][k]['precision'][1]=np.round(results[i][j][k]['precision'][1], 2)\n",
    "            results[i][j][k]['recall'][0]=np.round(results[i][j][k]['recall'][0], 2)\n",
    "            results[i][j][k]['recall'][1]=np.round(results[i][j][k]['recall'][1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table*}[h] \n",
      " \\centering \n",
      " \\caption{Aggregate analysis of evaluation metrics} \n",
      "  \\begin{tabular}{c|c|c|ccc|ccc|c} \n",
      "  \\hline \n",
      " \\multicolumn{3}{c}{} & \\multicolumn{3}{c}{Macro averaging} & \\multicolumn{3}{c}{Micro averaging} & \\multicolumn{1}{c}{} \\\\ \n",
      " \\hline \n",
      "  Classifier & Feature extraction & Accuracy & F1 Score & Precision & Recall & F1 Score & Precision & Recall \\\\ \n",
      "  \\hline \n",
      " \\multirow{4}{*}{LSTM}   & W2V/CNN &0.93 $ \\pm $ 0.01&0.88 $ \\pm $ 0.01&0.92 $ \\pm $ 0.01&0.85 $ \\pm $ 0.02&0.92 $ \\pm $ 0.01&0.93 $ \\pm $ 0.01&0.93 $ \\pm $ 0.01\\\\ \n",
      "  & Doc2Vec &0.82 $ \\pm $ 0.01&0.76 $ \\pm $ 0.02&0.77 $ \\pm $ 0.02&0.75 $ \\pm $ 0.02&0.82 $ \\pm $ 0.01&0.82 $ \\pm $ 0.01&0.82 $ \\pm $ 0.01\\\\ \n",
      "  & TFIDF &0.9 $ \\pm $ 0.01&0.85 $ \\pm $ 0.01&0.85 $ \\pm $ 0.01&0.85 $ \\pm $ 0.02&0.9 $ \\pm $ 0.01&0.9 $ \\pm $ 0.01&0.9 $ \\pm $ 0.01\\\\ \n",
      "  & BERT &0.93 $ \\pm $ 0.01&0.89 $ \\pm $ 0.01&0.92 $ \\pm $ 0.01&0.87 $ \\pm $ 0.02&0.93 $ \\pm $ 0.01&0.93 $ \\pm $ 0.01&0.93 $ \\pm $ 0.01\\\\ \n",
      "  \\hline \n",
      " \\multirow{4}{*}{MLP}    & W2V &0.91 $ \\pm $ 0.01&0.87 $ \\pm $ 0.01&0.92 $ \\pm $ 0.01&0.84 $ \\pm $ 0.02&0.91 $ \\pm $ 0.01&0.91 $ \\pm $ 0.01&0.91 $ \\pm $ 0.01\\\\ \n",
      "  & Doc2Vec &0.81 $ \\pm $ 0.01&0.76 $ \\pm $ 0.02&0.78 $ \\pm $ 0.02&0.74 $ \\pm $ 0.02&0.81 $ \\pm $ 0.01&0.81 $ \\pm $ 0.01&0.81 $ \\pm $ 0.01\\\\ \n",
      "  & TFIDF &0.92 $ \\pm $ 0.01&0.87 $ \\pm $ 0.01&0.93 $ \\pm $ 0.01&0.84 $ \\pm $ 0.02&0.92 $ \\pm $ 0.01&0.92 $ \\pm $ 0.01&0.92 $ \\pm $ 0.01\\\\ \n",
      "  & BERT &0.89 $ \\pm $ 0.01&0.83 $ \\pm $ 0.02&0.91 $ \\pm $ 0.01&0.79 $ \\pm $ 0.02&0.89 $ \\pm $ 0.01&0.89 $ \\pm $ 0.01&0.89 $ \\pm $ 0.01\\\\ \n",
      "  \\hline \n",
      " \\multirow{4}{*}{XGBoost}    & W2V &0.92 $ \\pm $ 0.01&0.87 $ \\pm $ 0.01&0.92 $ \\pm $ 0.01&0.84 $ \\pm $ 0.02&0.92 $ \\pm $ 0.01&0.92 $ \\pm $ 0.01&0.92 $ \\pm $ 0.01\\\\ \n",
      "  & Doc2Vec &0.87 $ \\pm $ 0.01&0.83 $ \\pm $ 0.01&0.89 $ \\pm $ 0.01&0.79 $ \\pm $ 0.02&0.87 $ \\pm $ 0.01&0.88 $ \\pm $ 0.01&0.87 $ \\pm $ 0.01\\\\ \n",
      "  & TFIDF &0.92 $ \\pm $ 0.01&0.88 $ \\pm $ 0.01&0.93 $ \\pm $ 0.01&0.84 $ \\pm $ 0.02&0.92 $ \\pm $ 0.01&0.93 $ \\pm $ 0.01&0.92 $ \\pm $ 0.01\\\\ \n",
      "  & BERT &0.92 $ \\pm $ 0.01&0.86 $ \\pm $ 0.01&0.92 $ \\pm $ 0.01&0.83 $ \\pm $ 0.02&0.92 $ \\pm $ 0.01&0.92 $ \\pm $ 0.01&0.92 $ \\pm $ 0.01\\\\ \n",
      " \\hline \n",
      "  \\end{tabular}% \n",
      " \\label{tab:lab}% \n",
      " \\end{table*}% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\\begin{table*}[h] \\n\",\n",
    "      \"\\centering \\n\",\n",
    "      '\\caption{Aggregate analysis of evaluation metrics} \\n',\n",
    "      ' \\\\begin{tabular}{c|c|c|ccc|ccc|c} \\n',\n",
    "      ' \\\\hline \\n',\n",
    "      '\\multicolumn{3}{c}{} & \\multicolumn{3}{c}{Macro averaging} & \\multicolumn{3}{c}{Micro averaging} & \\multicolumn{1}{c}{} \\\\\\ \\n',\n",
    "      '\\\\hline \\n',\n",
    "      ' Classifier & Feature extraction & Accuracy & F1 Score & Precision & Recall & F1 Score & Precision & Recall \\\\\\ \\n',\n",
    "      ' \\\\hline \\n',\n",
    "      '\\\\multirow{4}{*}{LSTM}   & W2V/CNN &'+str(results['lstm']['w2v']['accuracy'][0])+' $ \\pm $ '+str(results['lstm']['w2v']['accuracy'][1])+'&'+str(results['lstm']['w2v']['macro avg']['f1-score'][0])+' $ \\pm $ '+str(results['lstm']['w2v']['macro avg']['f1-score'][1])+'&'+str(results['lstm']['w2v']['macro avg']['precision'][0])+' $ \\pm $ '+str(results['lstm']['w2v']['macro avg']['precision'][1])+'&'+str(results['lstm']['w2v']['macro avg']['recall'][0])+' $ \\pm $ '+str(results['lstm']['w2v']['macro avg']['recall'][1])+'&'+str(results['lstm']['w2v']['weighted avg']['f1-score'][0])+' $ \\pm $ '+str(results['lstm']['w2v']['weighted avg']['f1-score'][1])+'&'+str(results['lstm']['w2v']['weighted avg']['precision'][0])+' $ \\pm $ '+str(results['lstm']['w2v']['weighted avg']['precision'][1])+'&'+str(results['lstm']['w2v']['weighted avg']['recall'][0])+' $ \\pm $ '+str(results['lstm']['w2v']['weighted avg']['recall'][1])+'\\\\\\ \\n',\n",
    "                              ' & Doc2Vec &'+str(results['lstm']['d2v']['accuracy'][0])+' $ \\pm $ '+str(results['lstm']['d2v']['accuracy'][1])+'&'+str(results['lstm']['d2v']['macro avg']['f1-score'][0])+' $ \\pm $ '+str(results['lstm']['d2v']['macro avg']['f1-score'][1])+'&'+str(results['lstm']['d2v']['macro avg']['precision'][0])+' $ \\pm $ '+str(results['lstm']['d2v']['macro avg']['precision'][1])+'&'+str(results['lstm']['d2v']['macro avg']['recall'][0])+' $ \\pm $ '+str(results['lstm']['d2v']['macro avg']['recall'][1])+'&'+str(results['lstm']['d2v']['weighted avg']['f1-score'][0])+' $ \\pm $ '+str(results['lstm']['d2v']['weighted avg']['f1-score'][1])+'&'+str(results['lstm']['d2v']['weighted avg']['precision'][0])+' $ \\pm $ '+str(results['lstm']['d2v']['weighted avg']['precision'][1])+'&'+str(results['lstm']['d2v']['weighted avg']['recall'][0])+' $ \\pm $ '+str(results['lstm']['d2v']['weighted avg']['recall'][1])+'\\\\\\ \\n',\n",
    "                              ' & TFIDF &'+str(results['lstm']['tfidf']['accuracy'][0])+' $ \\pm $ '+str(results['lstm']['tfidf']['accuracy'][1])+'&'+str(results['lstm']['tfidf']['macro avg']['f1-score'][0])+' $ \\pm $ '+str(results['lstm']['tfidf']['macro avg']['f1-score'][1])+'&'+str(results['lstm']['tfidf']['macro avg']['precision'][0])+' $ \\pm $ '+str(results['lstm']['tfidf']['macro avg']['precision'][1])+'&'+str(results['lstm']['tfidf']['macro avg']['recall'][0])+' $ \\pm $ '+str(results['lstm']['tfidf']['macro avg']['recall'][1])+'&'+str(results['lstm']['tfidf']['weighted avg']['f1-score'][0])+' $ \\pm $ '+str(results['lstm']['tfidf']['weighted avg']['f1-score'][1])+'&'+str(results['lstm']['tfidf']['weighted avg']['precision'][0])+' $ \\pm $ '+str(results['lstm']['tfidf']['weighted avg']['precision'][1])+'&'+str(results['lstm']['tfidf']['weighted avg']['recall'][0])+' $ \\pm $ '+str(results['lstm']['tfidf']['weighted avg']['recall'][1])+'\\\\\\ \\n',\n",
    "                              ' & BERT &'+str(results['lstm']['bert']['accuracy'][0])+' $ \\pm $ '+str(results['lstm']['bert']['accuracy'][1])+'&'+str(results['lstm']['bert']['macro avg']['f1-score'][0])+' $ \\pm $ '+str(results['lstm']['bert']['macro avg']['f1-score'][1])+'&'+str(results['lstm']['bert']['macro avg']['precision'][0])+' $ \\pm $ '+str(results['lstm']['bert']['macro avg']['precision'][1])+'&'+str(results['lstm']['bert']['macro avg']['recall'][0])+' $ \\pm $ '+str(results['lstm']['bert']['macro avg']['recall'][1])+'&'+str(results['lstm']['bert']['weighted avg']['f1-score'][0])+' $ \\pm $ '+str(results['lstm']['bert']['weighted avg']['f1-score'][1])+'&'+str(results['lstm']['bert']['weighted avg']['precision'][0])+' $ \\pm $ '+str(results['lstm']['bert']['weighted avg']['precision'][1])+'&'+str(results['lstm']['bert']['weighted avg']['recall'][0])+' $ \\pm $ '+str(results['lstm']['bert']['weighted avg']['recall'][1])+'\\\\\\ \\n',\n",
    "      ' \\\\hline \\n',\n",
    "      '\\\\multirow{4}{*}{MLP}    & W2V &'+str(results['mlp']['w2v']['accuracy'][0])+' $ \\pm $ '+str(results['mlp']['w2v']['accuracy'][1])+'&'+str(results['mlp']['w2v']['macro avg']['f1-score'][0])+' $ \\pm $ '+str(results['lstm']['w2v']['macro avg']['f1-score'][1])+'&'+str(results['lstm']['w2v']['macro avg']['precision'][0])+' $ \\pm $ '+str(results['mlp']['w2v']['macro avg']['precision'][1])+'&'+str(results['mlp']['w2v']['macro avg']['recall'][0])+' $ \\pm $ '+str(results['mlp']['w2v']['macro avg']['recall'][1])+'&'+str(results['mlp']['w2v']['weighted avg']['f1-score'][0])+' $ \\pm $ '+str(results['mlp']['w2v']['weighted avg']['f1-score'][1])+'&'+str(results['mlp']['w2v']['weighted avg']['precision'][0])+' $ \\pm $ '+str(results['mlp']['w2v']['weighted avg']['precision'][1])+'&'+str(results['mlp']['w2v']['weighted avg']['recall'][0])+' $ \\pm $ '+str(results['mlp']['w2v']['weighted avg']['recall'][1])+'\\\\\\ \\n',\n",
    "                              ' & Doc2Vec &'+str(results['mlp']['d2v']['accuracy'][0])+' $ \\pm $ '+str(results['mlp']['d2v']['accuracy'][1])+'&'+str(results['mlp']['d2v']['macro avg']['f1-score'][0])+' $ \\pm $ '+str(results['mlp']['d2v']['macro avg']['f1-score'][1])+'&'+str(results['mlp']['d2v']['macro avg']['precision'][0])+' $ \\pm $ '+str(results['mlp']['d2v']['macro avg']['precision'][1])+'&'+str(results['mlp']['d2v']['macro avg']['recall'][0])+' $ \\pm $ '+str(results['mlp']['d2v']['macro avg']['recall'][1])+'&'+str(results['mlp']['d2v']['weighted avg']['f1-score'][0])+' $ \\pm $ '+str(results['mlp']['d2v']['weighted avg']['f1-score'][1])+'&'+str(results['mlp']['d2v']['weighted avg']['precision'][0])+' $ \\pm $ '+str(results['mlp']['d2v']['weighted avg']['precision'][1])+'&'+str(results['mlp']['d2v']['weighted avg']['recall'][0])+' $ \\pm $ '+str(results['mlp']['d2v']['weighted avg']['recall'][1])+'\\\\\\ \\n',\n",
    "                              ' & TFIDF &'+str(results['mlp']['tfidf']['accuracy'][0])+' $ \\pm $ '+str(results['mlp']['tfidf']['accuracy'][1])+'&'+str(results['mlp']['tfidf']['macro avg']['f1-score'][0])+' $ \\pm $ '+str(results['mlp']['tfidf']['macro avg']['f1-score'][1])+'&'+str(results['mlp']['tfidf']['macro avg']['precision'][0])+' $ \\pm $ '+str(results['mlp']['tfidf']['macro avg']['precision'][1])+'&'+str(results['mlp']['tfidf']['macro avg']['recall'][0])+' $ \\pm $ '+str(results['mlp']['tfidf']['macro avg']['recall'][1])+'&'+str(results['mlp']['tfidf']['weighted avg']['f1-score'][0])+' $ \\pm $ '+str(results['mlp']['tfidf']['weighted avg']['f1-score'][1])+'&'+str(results['mlp']['tfidf']['weighted avg']['precision'][0])+' $ \\pm $ '+str(results['mlp']['tfidf']['weighted avg']['precision'][1])+'&'+str(results['mlp']['tfidf']['weighted avg']['recall'][0])+' $ \\pm $ '+str(results['mlp']['tfidf']['weighted avg']['recall'][1])+'\\\\\\ \\n',\n",
    "                              ' & BERT &'+str(results['mlp']['bert']['accuracy'][0])+' $ \\pm $ '+str(results['mlp']['bert']['accuracy'][1])+'&'+str(results['mlp']['bert']['macro avg']['f1-score'][0])+' $ \\pm $ '+str(results['mlp']['bert']['macro avg']['f1-score'][1])+'&'+str(results['mlp']['bert']['macro avg']['precision'][0])+' $ \\pm $ '+str(results['mlp']['bert']['macro avg']['precision'][1])+'&'+str(results['mlp']['bert']['macro avg']['recall'][0])+' $ \\pm $ '+str(results['mlp']['bert']['macro avg']['recall'][1])+'&'+str(results['mlp']['bert']['weighted avg']['f1-score'][0])+' $ \\pm $ '+str(results['mlp']['bert']['weighted avg']['f1-score'][1])+'&'+str(results['mlp']['bert']['weighted avg']['precision'][0])+' $ \\pm $ '+str(results['mlp']['bert']['weighted avg']['precision'][1])+'&'+str(results['mlp']['bert']['weighted avg']['recall'][0])+' $ \\pm $ '+str(results['mlp']['bert']['weighted avg']['recall'][1])+'\\\\\\ \\n',\n",
    "      ' \\\\hline \\n',\n",
    "      '\\\\multirow{4}{*}{XGBoost}    & W2V &'+str(results['xgboost']['w2v']['accuracy'][0])+' $ \\pm $ '+str(results['xgboost']['w2v']['accuracy'][1])+'&'+str(results['xgboost']['w2v']['macro avg']['f1-score'][0])+' $ \\pm $ '+str(results['lstm']['w2v']['macro avg']['f1-score'][1])+'&'+str(results['lstm']['w2v']['macro avg']['precision'][0])+' $ \\pm $ '+str(results['xgboost']['w2v']['macro avg']['precision'][1])+'&'+str(results['xgboost']['w2v']['macro avg']['recall'][0])+' $ \\pm $ '+str(results['xgboost']['w2v']['macro avg']['recall'][1])+'&'+str(results['xgboost']['w2v']['weighted avg']['f1-score'][0])+' $ \\pm $ '+str(results['xgboost']['w2v']['weighted avg']['f1-score'][1])+'&'+str(results['xgboost']['w2v']['weighted avg']['precision'][0])+' $ \\pm $ '+str(results['xgboost']['w2v']['weighted avg']['precision'][1])+'&'+str(results['xgboost']['w2v']['weighted avg']['recall'][0])+' $ \\pm $ '+str(results['xgboost']['w2v']['weighted avg']['recall'][1])+'\\\\\\ \\n',\n",
    "                              ' & Doc2Vec &'+str(results['xgboost']['d2v']['accuracy'][0])+' $ \\pm $ '+str(results['xgboost']['d2v']['accuracy'][1])+'&'+str(results['xgboost']['d2v']['macro avg']['f1-score'][0])+' $ \\pm $ '+str(results['xgboost']['d2v']['macro avg']['f1-score'][1])+'&'+str(results['xgboost']['d2v']['macro avg']['precision'][0])+' $ \\pm $ '+str(results['xgboost']['d2v']['macro avg']['precision'][1])+'&'+str(results['xgboost']['d2v']['macro avg']['recall'][0])+' $ \\pm $ '+str(results['xgboost']['d2v']['macro avg']['recall'][1])+'&'+str(results['xgboost']['d2v']['weighted avg']['f1-score'][0])+' $ \\pm $ '+str(results['xgboost']['d2v']['weighted avg']['f1-score'][1])+'&'+str(results['xgboost']['d2v']['weighted avg']['precision'][0])+' $ \\pm $ '+str(results['xgboost']['d2v']['weighted avg']['precision'][1])+'&'+str(results['xgboost']['d2v']['weighted avg']['recall'][0])+' $ \\pm $ '+str(results['xgboost']['d2v']['weighted avg']['recall'][1])+'\\\\\\ \\n',\n",
    "                              ' & TFIDF &'+str(results['xgboost']['tfidf']['accuracy'][0])+' $ \\pm $ '+str(results['xgboost']['tfidf']['accuracy'][1])+'&'+str(results['xgboost']['tfidf']['macro avg']['f1-score'][0])+' $ \\pm $ '+str(results['xgboost']['tfidf']['macro avg']['f1-score'][1])+'&'+str(results['xgboost']['tfidf']['macro avg']['precision'][0])+' $ \\pm $ '+str(results['xgboost']['tfidf']['macro avg']['precision'][1])+'&'+str(results['xgboost']['tfidf']['macro avg']['recall'][0])+' $ \\pm $ '+str(results['xgboost']['tfidf']['macro avg']['recall'][1])+'&'+str(results['xgboost']['tfidf']['weighted avg']['f1-score'][0])+' $ \\pm $ '+str(results['xgboost']['tfidf']['weighted avg']['f1-score'][1])+'&'+str(results['xgboost']['tfidf']['weighted avg']['precision'][0])+' $ \\pm $ '+str(results['xgboost']['tfidf']['weighted avg']['precision'][1])+'&'+str(results['xgboost']['tfidf']['weighted avg']['recall'][0])+' $ \\pm $ '+str(results['xgboost']['tfidf']['weighted avg']['recall'][1])+'\\\\\\ \\n',\n",
    "                              ' & BERT &'+str(results['xgboost']['bert']['accuracy'][0])+' $ \\pm $ '+str(results['xgboost']['bert']['accuracy'][1])+'&'+str(results['xgboost']['bert']['macro avg']['f1-score'][0])+' $ \\pm $ '+str(results['xgboost']['bert']['macro avg']['f1-score'][1])+'&'+str(results['xgboost']['bert']['macro avg']['precision'][0])+' $ \\pm $ '+str(results['xgboost']['bert']['macro avg']['precision'][1])+'&'+str(results['xgboost']['bert']['macro avg']['recall'][0])+' $ \\pm $ '+str(results['xgboost']['bert']['macro avg']['recall'][1])+'&'+str(results['xgboost']['bert']['weighted avg']['f1-score'][0])+' $ \\pm $ '+str(results['xgboost']['bert']['weighted avg']['f1-score'][1])+'&'+str(results['xgboost']['bert']['weighted avg']['precision'][0])+' $ \\pm $ '+str(results['xgboost']['bert']['weighted avg']['precision'][1])+'&'+str(results['xgboost']['bert']['weighted avg']['recall'][0])+' $ \\pm $ '+str(results['xgboost']['bert']['weighted avg']['recall'][1])+'\\\\\\ \\n',\n",
    "      '\\\\hline \\n', \n",
    "      ' \\end{tabular}% \\n',\n",
    "      '\\label{tab:lab}% \\n',\n",
    "   '\\end{table*}% \\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
